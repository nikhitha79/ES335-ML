When independent variables in a regression model are highly correlated with each other, it leads to multicollinearity, which can cause issues in traditional regression models. In such cases, the matrix (X.T @ X) becomes singular or nearly singular, making it impossible or unstable to compute its inverse. This arises because multicollinearity results in linear dependence among the columns of the design matrix X, causing it to lose full rank.

Sklearn's linear regression implementation offers regularization techniques like Ridge Regression and Lasso Regression to address multicollinearity. These techniques introduce penalty terms to the loss function, which discourages large coefficients during parameter estimation. By adding a small penalty term to the diagonal elements of the (X.T @ X) matrix, Sklearn's linear regression implementation ensures that the matrix becomes invertible even in the presence of multicollinearity. This regularization effectively reduces the magnitudes of the coefficients, making the model less sensitive to collinear features and helping to mitigate multicollinearity issues.
